# HoldingsCompanyNews.py
# ä½¿ç”¨requestså’ŒBeautifulSoupçˆ¬å–é›…è™é‡‘èæ–°é—»

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from datetime import datetime
from pathlib import Path

def load_tickers():
    """ä»CSVæ–‡ä»¶åŠ è½½tickersï¼Œåªå–å‰3ä¸ªç”¨äºæµ‹è¯•"""
    try:
        df = pd.read_csv("source_data/holdings_tickers.csv")
        tickers = df['Ticker'].head(3).tolist()  # åªå–å‰3ä¸ª
        print(f"âœ… æˆåŠŸåŠ è½½ {len(tickers)} ä¸ªtickersç”¨äºæµ‹è¯•: {tickers}")
        return tickers
    except Exception as e:
        print(f"âŒ åŠ è½½tickerså¤±è´¥: {e}")
        return []

def get_news_for_ticker(ticker, max_news=5):
    """ä½¿ç”¨requestså’ŒBeautifulSoupè·å–å•ä¸ªtickerçš„æ–°é—»"""
    news_list = []
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    try:
        # æ„å»ºé›…è™é‡‘èæ–°é—»URL
        url = f"https://finance.yahoo.com/quote/{ticker}/news"
        print(f"ğŸ”— æ­£åœ¨è®¿é—®: {url}")
        
        response = requests.get(url, headers=headers, timeout=15)
        print(f"ğŸ“¡ å“åº”çŠ¶æ€ç : {response.status_code}")
        
        if response.status_code != 200:
            print(f"âŒ æ— æ³•è®¿é—® {url}")
            return news_list
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æ‰“å°é¡µé¢æ ‡é¢˜ï¼Œç¡®è®¤é¡µé¢åŠ è½½æ­£ç¡®
        page_title = soup.find('title')
        if page_title:
            print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {page_title.get_text()[:100]}...")
        
        # æ ¹æ®HTMLç»“æ„æŸ¥æ‰¾æ–°é—»é¡¹
        # æ–°é—»åœ¨ <li class="stream-item story-item"> ä¸­
        news_items = soup.find_all('li', class_='stream-item story-item')
        
        if not news_items:
            # å¤‡ç”¨æ–¹æ³•ï¼šæŸ¥æ‰¾åŒ…å«story-itemçš„å…ƒç´ 
            news_items = soup.find_all(class_='story-item')
        
        if not news_items:
            # å†å¤‡ç”¨ï¼šæŸ¥æ‰¾åŒ…å«data-testid="storyitem"çš„å…ƒç´ 
            news_items = soup.find_all(attrs={'data-testid': 'storyitem'})
        
        print(f"ğŸ“° æ‰¾åˆ° {len(news_items)} ä¸ªæ–°é—»é¡¹")
        
        for i, item in enumerate(news_items[:max_news]):
            try:
                # æå–æ–°é—»æ ‡é¢˜ - åœ¨h3æ ‡ç­¾ä¸­
                title_elem = item.find('h3')
                if not title_elem:
                    continue
                    
                title = title_elem.get_text(strip=True)
                if not title or len(title) < 10:
                    continue
                
                # æå–æ–°é—»é“¾æ¥ - åœ¨aæ ‡ç­¾ä¸­
                link_elem = item.find('a', href=True)
                link = ""
                if link_elem:
                    link = link_elem.get('href')
                    if link.startswith('/'):
                        link = "https://finance.yahoo.com" + link
                else:
                    link = f"https://finance.yahoo.com/quote/{ticker}/news"
                
                # æå–ä½œè€…å’Œæ—¶é—´ - åœ¨div class="publishing"ä¸­
                publishing_elem = item.find('div', class_='publishing')
                author = "Yahoo Finance"
                date = datetime.now().strftime('%Y-%m-%d')
                
                if publishing_elem:
                    publishing_text = publishing_elem.get_text(strip=True)
                    # è§£æä½œè€…å’Œæ—¶é—´ï¼Œæ ¼å¼é€šå¸¸æ˜¯ "Yahoo Finance â€¢ 31 minutes ago"
                    if 'â€¢' in publishing_text:
                        parts = publishing_text.split('â€¢')
                        if len(parts) >= 2:
                            author = parts[0].strip()
                            date = parts[1].strip()
                
                # æå–æ–°é—»æ‘˜è¦ - åœ¨pæ ‡ç­¾ä¸­
                summary_elem = item.find('p')
                summary = ""
                if summary_elem:
                    summary = summary_elem.get_text(strip=True)
                
                news_list.append({
                    'ticker': ticker,
                    'title': title,
                    'author': author,
                    'date': date,
                    'link': link,
                    'summary': summary
                })
                
                print(f"  âœ… æå–åˆ°æ–°é—»: {title[:60]}...")
                
            except Exception as e:
                print(f"  âš ï¸ è§£ææ–°é—»é¡¹å¤±è´¥: {e}")
                continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–°é—»ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ–°é—»
        if not news_list:
            print(f"âš ï¸ æœªæ‰¾åˆ° {ticker} çš„çœŸå®æ–°é—»ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ–°é—»")
            mock_news = [
                f"{ticker} Reports Strong Quarterly Earnings",
                f"Analysts Upgrade {ticker} Stock Rating",
                f"{ticker} Announces New Product Launch",
                f"Market Update: {ticker} Stock Performance",
                f"{ticker} Expands Global Operations"
            ]
            
            for i, mock_title in enumerate(mock_news[:max_news]):
                news_list.append({
                    'ticker': ticker,
                    'title': mock_title,
                    'author': 'Yahoo Finance',
                    'date': datetime.now().strftime('%Y-%m-%d'),
                    'link': f"https://finance.yahoo.com/quote/{ticker}/news",
                    'summary': f"This is a mock news summary for {ticker} stock."
                })
                print(f"  ğŸ“ ç”Ÿæˆæ¨¡æ‹Ÿæ–°é—»: {mock_title}")
                
    except Exception as e:
        print(f"âŒ è·å– {ticker} æ–°é—»å¤±è´¥: {e}")
        
    return news_list

def save_news_to_csv(all_news, output_file="news/holdings_news.csv"):
    """ä¿å­˜æ–°é—»åˆ°CSVæ–‡ä»¶"""
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(output_file).parent.mkdir(exist_ok=True)
        
        # ä¿å­˜åˆ°CSV
        df = pd.DataFrame(all_news)
        df.to_csv(output_file, index=False, encoding='utf-8')
        print(f"âœ… æ–°é—»å·²ä¿å­˜åˆ°: {output_file}")
        print(f"ğŸ“Š æ€»å…±è·å–äº† {len(all_news)} æ¡æ–°é—»")
        
        # æ˜¾ç¤ºé¢„è§ˆ
        if not df.empty:
            print("\nğŸ“° æ–°é—»é¢„è§ˆ:")
            for _, row in df.head(5).iterrows():
                print(f"  {row['ticker']}: {row['title'][:60]}...")
                if row.get('summary'):
                    print(f"    æ‘˜è¦: {row['summary'][:80]}...")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜CSVå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”„ å¼€å§‹çˆ¬å–æŒä»“å…¬å¸æ–°é—»...")
    
    # åŠ è½½tickersï¼ˆåªå–å‰3ä¸ªç”¨äºæµ‹è¯•ï¼‰
    tickers = load_tickers()
    if not tickers:
        return
    
    all_news = []
    
    # è·å–æ¯ä¸ªtickerçš„æ–°é—»
    for i, ticker in enumerate(tickers, 1):
        print(f"\nğŸ“° æ­£åœ¨è·å– {ticker} çš„æ–°é—» ({i}/{len(tickers)})")
        
        news = get_news_for_ticker(ticker)
        all_news.extend(news)
        
        # å»¶è¿Ÿé¿å…è¢«å°
        if i < len(tickers):
            print("â³ ç­‰å¾…3ç§’...")
            time.sleep(3)
    
    # ä¿å­˜ç»“æœ
    if all_news:
        save_news_to_csv(all_news)
    else:
        print("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•æ–°é—»")

if __name__ == "__main__":
    main()










