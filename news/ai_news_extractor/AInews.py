from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import json
import os
from datetime import datetime, timedelta
import re

def save_news_to_file(sections, date_text, filename=None):
    """ä¿å­˜æ–°é—»å†…å®¹åˆ°æ–‡ä»¶"""
    if not filename:
        # ç”Ÿæˆæ–‡ä»¶å
        date_obj = datetime.now()
        filename = f"ai_news_{date_obj.strftime('%Y%m%d')}.json"
    
    # ç¡®ä¿dataç›®å½•å­˜åœ¨
    os.makedirs('data', exist_ok=True)
    filepath = os.path.join('data', filename)
    
    # å‡†å¤‡ä¿å­˜çš„æ•°æ®
    news_data = {
        "date": date_text,
        "extracted_at": datetime.now().isoformat(),
        "sections": sections,
        "total_sections": len(sections)
    }
    
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(news_data, f, ensure_ascii=False, indent=2)
        print(f"æ–°é—»å†…å®¹å·²ä¿å­˜åˆ°: {filepath}")
        return filepath
    except Exception as e:
        print(f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None

def format_content_for_display(content, max_length=800):
    """æ ¼å¼åŒ–å†…å®¹ç”¨äºæ˜¾ç¤º"""
    if len(content) <= max_length:
        return content
    
    # åœ¨å¥å­è¾¹ç•Œæˆªæ–­
    truncated = content[:max_length]
    last_period = truncated.rfind('.')
    last_newline = truncated.rfind('\n')
    
    if last_period > last_newline and last_period > max_length * 0.8:
        return truncated[:last_period + 1] + "\n[å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­]"
    elif last_newline > max_length * 0.8:
        return truncated[:last_newline] + "\n[å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­]"
    else:
        return truncated + "\n[å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­]"

def get_yesterday_link(driver, base_url):
    """è·å–æ˜¨æ—¥çš„æ–°é—»é“¾æ¥"""
    try:
        # æ‰“å¼€ä¸»é¡µ
        driver.get(base_url)
        
        # ç­‰å¾…å†…å®¹åŠ è½½
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "li.relative.md\\:pl-10"))
        )
        time.sleep(2)
        
        # è·å–æ‰€æœ‰æ–°é—»æ¡ç›®
        news_items = driver.find_elements(By.CSS_SELECTOR, "li.relative.md\\:pl-10")
        
        # è·å–æ˜¨æ—¥æ—¥æœŸ
        yesterday = datetime.now() - timedelta(days=1)
        yesterday_str = yesterday.strftime("%m-%d")  # æ ¼å¼å¦‚ "07-23"
        
        # æŸ¥æ‰¾æ˜¨æ—¥é“¾æ¥
        for item in news_items:
            date_element = item.find_element(By.CSS_SELECTOR, "time")
            date_text = date_element.text
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ˜¨æ—¥
            if yesterday_str in date_text or "yesterday" in date_text.lower():
                link_element = item.find_element(By.CSS_SELECTOR, "a.block")
                relative_link = link_element.get_attribute("href")
                full_link = relative_link if relative_link.startswith("http") else base_url + relative_link
                return full_link, date_text
        
        # å¦‚æœæ²¡æ‰¾åˆ°æ˜¨æ—¥ï¼Œè¿”å›ç¬¬ä¸€æ¡
        if news_items:
            link_element = news_items[0].find_element(By.CSS_SELECTOR, "a.block")
            relative_link = link_element.get_attribute("href")
            full_link = relative_link if relative_link.startswith("http") else base_url + relative_link
            date_element = news_items[0].find_element(By.CSS_SELECTOR, "time")
            return full_link, date_element.text
            
    except Exception as e:
        print(f"è·å–æ˜¨æ—¥é“¾æ¥æ—¶å‡ºé”™: {e}")
        return None, None

def extract_from_specific_url(url, date_text=None):
    """ä»æŒ‡å®šURLæå–AIæ–°é—»å†…å®¹"""
    driver = webdriver.Chrome()
    
    try:
        print(f"æ­£åœ¨ä»æŒ‡å®šURLæå–å†…å®¹: {url}")
        news_sections = extract_ai_news_content(driver, url)
        
        if not date_text:
            date_text = "æŒ‡å®šæ—¥æœŸ"
        
        # è¾“å‡ºç»“æœ
        print(f"\n{'='*60}")
        print(f"ğŸ“° {date_text} AIæ–°é—»æ±‡æ€»")
        print(f"{'='*60}")
        
        if not news_sections:
            print("âŒ æœªèƒ½æå–åˆ°ä»»ä½•æ–°é—»å†…å®¹")
            return
        
        for i, (section_name, content) in enumerate(news_sections.items(), 1):
            print(f"\nğŸ“‹ {i}. {section_name}")
            print(f"{'-'*50}")
            
            # æ ¼å¼åŒ–æ˜¾ç¤ºå†…å®¹
            formatted_content = format_content_for_display(content)
            print(formatted_content)
            
            # æ˜¾ç¤ºå†…å®¹é•¿åº¦
            print(f"\nğŸ“Š å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            print("-" * 50)
        
        print(f"\nâœ… æˆåŠŸæå–äº† {len(news_sections)} ä¸ªæ–°é—»éƒ¨åˆ†")
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        save_news_to_file(news_sections, date_text)
        
        return news_sections
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        return None
        
    finally:
        driver.quit()

def extract_ai_news_content(driver, url):
    """æå–AIæ–°é—»é¡µé¢çš„å…·ä½“å†…å®¹"""
    try:
        # æ‰“å¼€æ–°é—»é¡µé¢
        driver.get(url)
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        time.sleep(3)
        
        # è·å–é¡µé¢å†…å®¹
        page_content = driver.find_element(By.TAG_NAME, "body").text
        
        # æå–ä¸»è¦éƒ¨åˆ†
        sections = {}
        
        # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼æå–å„ä¸ªéƒ¨åˆ†
        # æå–AI Twitter Recap - æ”¹è¿›æ¨¡å¼
        twitter_patterns = [
            r'AI Twitter Recap\s*(.*?)(?=\s*AI Reddit Recap|\s*AI Discord Recap|\s*Discord:|$)',
            r'Twitter Recap\s*(.*?)(?=\s*Reddit Recap|\s*Discord Recap|\s*Discord:|$)',
            r'Twitter\s*(.*?)(?=\s*Reddit|\s*Discord|$)'
        ]
        
        for pattern in twitter_patterns:
            twitter_match = re.search(pattern, page_content, re.DOTALL | re.IGNORECASE)
            if twitter_match:
                twitter_content = twitter_match.group(1).strip()
                if len(twitter_content) > 20:  # ç¡®ä¿å†…å®¹æœ‰æ„ä¹‰
                    sections['AI Twitter Recap'] = twitter_content
                    break
        
        # æå–AI Reddit Recap
        reddit_patterns = [
            r'AI Reddit Recap\s*(.*?)(?=\s*AI Discord Recap|\s*Discord:|$)',
            r'Reddit Recap\s*(.*?)(?=\s*Discord Recap|\s*Discord:|$)',
            r'Reddit\s*(.*?)(?=\s*Discord|$)'
        ]
        
        for pattern in reddit_patterns:
            reddit_match = re.search(pattern, page_content, re.DOTALL | re.IGNORECASE)
            if reddit_match:
                reddit_content = reddit_match.group(1).strip()
                if len(reddit_content) > 20:
                    sections['AI Reddit Recap'] = reddit_content
                    break
        
        # æå–AI Discord Recap
        discord_patterns = [
            r'AI Discord Recap\s*(.*?)(?=\s*AI Twitter Recap|\s*AI Reddit Recap|$)',
            r'Discord Recap\s*(.*?)(?=\s*Twitter Recap|\s*Reddit Recap|$)',
            r'Discord:\s*(.*?)(?=\s*Twitter|\s*Reddit|$)'
        ]
        
        for pattern in discord_patterns:
            discord_match = re.search(pattern, page_content, re.DOTALL | re.IGNORECASE)
            if discord_match:
                discord_content = discord_match.group(1).strip()
                if len(discord_content) > 20:
                    sections['AI Discord Recap'] = discord_content
                    break
        
        # å¦‚æœæ­£åˆ™è¡¨è¾¾å¼æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿçš„å†…å®¹ï¼Œå°è¯•ä½¿ç”¨DOMç»“æ„
        if len(sections) < 2:
            print("ä½¿ç”¨DOMç»“æ„æå–å†…å®¹...")
            
            # æŸ¥æ‰¾æ‰€æœ‰æ ‡é¢˜å…ƒç´ 
            headings = driver.find_elements(By.CSS_SELECTOR, "h1, h2, h3, h4, h5, h6")
            
            for heading in headings:
                heading_text = heading.text.strip()
                if not heading_text:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬è¦æ‰¾çš„æ ‡é¢˜
                if any(keyword in heading_text.lower() for keyword in 
                       ['twitter recap', 'reddit recap', 'discord recap', 'recap']):
                    
                    # è·å–è¯¥æ ‡é¢˜ä¸‹çš„å†…å®¹
                    try:
                        # æ–¹æ³•1ï¼šæŸ¥æ‰¾ä¸‹ä¸€ä¸ªå…„å¼Ÿå…ƒç´ 
                        parent = heading.find_element(By.XPATH, "..")
                        siblings = parent.find_elements(By.XPATH, "following-sibling::*")
                        
                        content_parts = []
                        for sibling in siblings[:10]:  # é™åˆ¶æŸ¥æ‰¾èŒƒå›´
                            if sibling.tag_name in ['p', 'div', 'section']:
                                sibling_text = sibling.text.strip()
                                if sibling_text and len(sibling_text) > 10:
                                    content_parts.append(sibling_text)
                        
                        if content_parts:
                            sections[heading_text] = '\n\n'.join(content_parts)
                            
                    except Exception as e:
                        print(f"æå– {heading_text} å†…å®¹æ—¶å‡ºé”™: {e}")
                        continue
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰è¶³å¤Ÿå†…å®¹ï¼Œå°è¯•æå–æ•´ä¸ªé¡µé¢çš„ä¸»è¦æ–‡æœ¬
        if len(sections) < 1:
            print("æå–é¡µé¢ä¸»è¦æ–‡æœ¬å†…å®¹...")
            
            # æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
            main_content = driver.find_elements(By.CSS_SELECTOR, "main, article, .content, .post-content")
            if main_content:
                main_text = main_content[0].text
                sections['å®Œæ•´å†…å®¹'] = main_text[:2000] + "..." if len(main_text) > 2000 else main_text
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸï¼Œä½¿ç”¨bodyæ–‡æœ¬
                body_text = driver.find_element(By.TAG_NAME, "body").text
                sections['é¡µé¢å†…å®¹'] = body_text[:2000] + "..." if len(body_text) > 2000 else body_text
        
        return sections
        
    except Exception as e:
        print(f"æå–æ–°é—»å†…å®¹æ—¶å‡ºé”™: {e}")
        return {}

def main():
    # åˆå§‹åŒ–æµè§ˆå™¨
    driver = webdriver.Chrome()
    base_url = "https://news.smol.ai"
    
    try:
        # 1. è·å–æ˜¨æ—¥é“¾æ¥
        print("æ­£åœ¨è·å–æ˜¨æ—¥æ–°é—»é“¾æ¥...")
        yesterday_link, date_text = get_yesterday_link(driver, base_url)
        
        if not yesterday_link:
            print("æœªèƒ½è·å–åˆ°æ˜¨æ—¥é“¾æ¥")
            return
        
        print(f"æ‰¾åˆ°æ˜¨æ—¥é“¾æ¥: {yesterday_link}")
        print(f"æ—¥æœŸ: {date_text}")
        
        # 2. æå–æ–°é—»å†…å®¹
        print("\næ­£åœ¨æå–AIæ–°é—»å†…å®¹...")
        news_sections = extract_ai_news_content(driver, yesterday_link)
        
        # 3. è¾“å‡ºç»“æœ
        print(f"\n{'='*60}")
        print(f"ğŸ“° {date_text} AIæ–°é—»æ±‡æ€»")
        print(f"{'='*60}")
        
        if not news_sections:
            print("âŒ æœªèƒ½æå–åˆ°ä»»ä½•æ–°é—»å†…å®¹")
            return
        
        for i, (section_name, content) in enumerate(news_sections.items(), 1):
            print(f"\nğŸ“‹ {i}. {section_name}")
            print(f"{'-'*50}")
            
            # æ ¼å¼åŒ–æ˜¾ç¤ºå†…å®¹
            formatted_content = format_content_for_display(content)
            print(formatted_content)
            
            # æ˜¾ç¤ºå†…å®¹é•¿åº¦
            print(f"\nğŸ“Š å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            print("-" * 50)
        
        print(f"\nâœ… æˆåŠŸæå–äº† {len(news_sections)} ä¸ªæ–°é—»éƒ¨åˆ†")
        
        # 4. ä¿å­˜åˆ°æ–‡ä»¶
        save_news_to_file(news_sections, date_text)
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        
    finally:
        # å…³é—­æµè§ˆå™¨
        driver.quit()

if __name__ == "__main__":
    # å¯ä»¥é€‰æ‹©è¿è¡Œä¸»å‡½æ•°æˆ–ç›´æ¥å¤„ç†æŒ‡å®šURL
    import sys
    
    if len(sys.argv) > 1:
        # å¦‚æœæä¾›äº†URLå‚æ•°ï¼Œç›´æ¥å¤„ç†è¯¥URL
        url = sys.argv[1]
        date_text = sys.argv[2] if len(sys.argv) > 2 else "æŒ‡å®šæ—¥æœŸ"
        extract_from_specific_url(url, date_text)
    else:
        # å¦åˆ™è¿è¡Œä¸»å‡½æ•°è·å–æ˜¨æ—¥æ–°é—»
        main()